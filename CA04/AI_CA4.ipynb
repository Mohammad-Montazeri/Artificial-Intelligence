{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color:#FFFDD0; border-radius:18px; padding:15px; \">\n",
        "<h4 style=\"margin-left: 8%; color:#000130; \"> CA03  -  <i>Artificial Intelligence Course</i>  -      Dr. Fadaei \n",
        "<br>\n",
        "<b> Mohammad Montazeri - 810699269 </b> - Fall 1402 </h4>\n",
        "\n",
        "<h1 style=\"text-align: center; color:#000130; \"> Reinforcement Learning </h1>\n",
        "<br>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksnYjMyNPAcn"
      },
      "source": [
        "# Table of Contents\n",
        "* [Part 1: Value Iteration & Policy Iteration Algorithms](#1)\n",
        "    - [َQuestion 1:](#1-0)\n",
        "    - [َQuestion 2:](#1-1)\n",
        "    - [َQuestion 3:](#1-12)\n",
        "    - [َQuestion 4:](#1-2)\n",
        "    - [َQuestion 5:](#1-3)\n",
        "        - [Value Iteration](#1-3-1)\n",
        "        - [Policy Iteration](#1-3-2)\n",
        "    - [َQuestion 6:](#1-4)\n",
        "        - [Value Iteration](#1-4-1)\n",
        "        - [Policy Iteration](#1-4-2)\n",
        "* [Part 2: Q-Learning Algorithm](#2)\n",
        "    - [َQuestion 7:](#2-0)\n",
        "    - [َQuestion 8:](#2-1)\n",
        "    - [َQuestion 9:](#2-2)\n",
        "    - [َQuestion 10:](#2-3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpTWKluXMHP5",
        "outputId": "c8bc002c-6d33-4cd8-84af-44a41dc57bd1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EifP8FUKLXE7"
      },
      "source": [
        "<a name='1'></a>\n",
        "\n",
        "## Part 1: Value Iteration & Policy Iteration Algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8LizJeOYRMEq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "you can see the environment in each step by render command\n",
            "16\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "# Defining the hyperparameters of the problem\n",
        "axes_states = 4\n",
        "total_states = axes_states * axes_states\n",
        "discount_factor = 0.9\n",
        "iterations = 100\n",
        "action_symbols = [\"←\", \"↓\", \"→\", \"↑\"]\n",
        "action_space = {0: \"LEFT\", 1: \"DOWN\", 2: \"RIGHT\", 3: \"UP\"}\n",
        "\n",
        "# Introducing the environment of the problem\n",
        "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\",\n",
        "               is_slippery=False, new_step_api=True)\n",
        "\n",
        "# Getting familiar with the environment\n",
        "print(\"you can see the environment in each step by render command\")\n",
        "initial_state = env.reset()\n",
        "# env.render()\n",
        "\n",
        "# Total no. of states\n",
        "print(env.observation_space.n)\n",
        "\n",
        "# Total no. of actions\n",
        "print(env.action_space.n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVJmGmCUnIGR"
      },
      "source": [
        "<a name='1-0'></a>\n",
        "\n",
        "### Question 1:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRDhQMwwnK2s"
      },
      "source": [
        "### Value Iteration Method\n",
        "\n",
        "The Value Iteration method is a dynamic programming algorithm applied in Reinforcement Learning (RL) to determine the optimal value function and policy. Through iterative updates, the algorithm refines the value assigned to each state in the environment until reaching convergence, indicating stabilization where state values exhibit minimal change between iterations.  \n",
        "The procedural steps of the Value Iteration algorithm are outlined as follows:\n",
        "\n",
        "1. Initialize the value of each state to 0.\n",
        "2. For each state, compute the expected return by evaluating all possible actions. This expected return is the sum of the reward and the discounted value of the subsequent state for each potential action, weighted by the probability associated with that action.\n",
        "3. Update the state's value to be the maximum expected return.\n",
        "4. Repeat steps 2 and 3 until the value of each state converges, defined as the maximum difference in value between iterations being less than a predefined small threshold.\n",
        "\n",
        "The Bellman equation serves as the mechanism to update the value of each state. This recursive equation expresses the state's value as the sum of the immediate reward and the discounted value of the next state, weighted by the probability of transitioning to that next state. Here is the equation:\n",
        "$$V(s) = \\max_{a} \\sum_{s', r} p(s', r|s, a)[r + \\gamma V(s')]$$\n",
        "where:\n",
        "\n",
        "* $V(s)$ is the value of state $s$\n",
        "* $a$ is the action taken\n",
        "* $s'$ is the resulting state\n",
        "* $r$ is the reward received\n",
        "* $p(s', r|s, a)$ is the probability of transitioning to state $s'$ and receiving reward $r$ from state $s$ when taking action $a$\n",
        "* $\\gamma$ is the discount factor that trades off immediate versus future rewards\n",
        "\n",
        "### Attaining the Optimal Policy via Value Iteration\n",
        "\n",
        "Upon convergence of the value function, the optimal policy can be derived by selecting the action that maximizes the expected return for each state. This optimal policy serves as a mapping from each state to the action that yields the highest expected return. Consequently, this action represents the one offering the maximum expected reward when commencing from the given state and adhering to the optimal policy thereafter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO24LtBGLXZ7"
      },
      "source": [
        "<a name='1-1'></a>\n",
        "\n",
        "### Question 2:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VKP8LjK5jGoW"
      },
      "outputs": [],
      "source": [
        "class ValueIteration:\n",
        "    def __init__(self, env, discount_factor, theta=1e-8):\n",
        "        self.env = env\n",
        "        self.discount_factor = discount_factor\n",
        "        self.theta = theta\n",
        "        self.reset()\n",
        "        self.state_values = (\n",
        "            np.ones((self.env.observation_space.n)) / self.env.action_space.n\n",
        "        )\n",
        "        self.q_values = (\n",
        "            np.ones((self.env.observation_space.n, self.env.action_space.n))\n",
        "            / self.env.action_space.n\n",
        "        )\n",
        "        self.state_values[self.env.observation_space.n - 1] = 0\n",
        "        self.q_values[self.env.observation_space.n - 1] = np.zeros(\n",
        "            (self.env.action_space.n)\n",
        "        )\n",
        "\n",
        "    def value_estimation(self):\n",
        "        self.delta = np.inf\n",
        "\n",
        "        while self.delta > self.theta:\n",
        "            self.delta = 0\n",
        "\n",
        "            for state in range(self.env.observation_space.n):\n",
        "                v = self.state_values[state]\n",
        "\n",
        "                for action in range(self.env.action_space.n):\n",
        "                    action_value = 0\n",
        "                    for probability, next_state, reward, done in self.env.P[state][\n",
        "                        action\n",
        "                    ]:\n",
        "                        if done:\n",
        "                            action_value += reward\n",
        "                        else:\n",
        "                            action_value += probability * (\n",
        "                                reward\n",
        "                                + self.discount_factor *\n",
        "                                self.state_values[next_state]\n",
        "                            )\n",
        "\n",
        "                    self.q_values[state, action] = action_value\n",
        "\n",
        "                self.state_values[state] = np.max(self.q_values[state, :])\n",
        "\n",
        "                self.delta = np.max(\n",
        "                    [self.delta, abs(v - self.state_values[state])])\n",
        "\n",
        "    def take_action(self, action):\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def get_optimal_policy(self, state):\n",
        "        return np.argmax(self.q_values[state, :])\n",
        "\n",
        "    def get_state_values(self):\n",
        "        return self.state_values\n",
        "\n",
        "    def get_q_values(self):\n",
        "        return self.q_values\n",
        "\n",
        "    def reset(self):\n",
        "        initial_state = self.env.reset()\n",
        "        return initial_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frjc5mR4ncm1"
      },
      "source": [
        "<a name='1-12'></a>\n",
        "\n",
        "### Question 3:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMJJpf1tnddF"
      },
      "source": [
        "### Policy Iteration Method\n",
        "\n",
        "In RL, a **policy** acts as a strategy dictating the agent's behavior at any given moment, essentially functioning as a mapping from *states* to *actions*. Policy Iteration serves as a crucial algorithm in Reinforcement Learning (RL) aimed at discovering the optimal policy. The Policy Iteration algorithm unfolds in two distinct steps as:\n",
        "\n",
        "1. **Policy Evaluation** which involves assessing the existing policy by computing the expected return (or value function) for each state under the current policy. The expected return for a state is the cumulative sum of rewards obtained by adhering to the policy from that specific state onward. This iterative process continues until the value function converges to its optimal counterpart.\n",
        "2. **Policy Improvement** which causes the current policy undergo an enhancement by selecting a new action for each state, aiming to maximize the expected return. This iterative refinement persists until the policy stabilizes, signifying convergence to the optimal policy. The algorithm's complexity is directly proportional to the number of states, actions, and iterations essential for convergence. The maximum iteration count corresponds to the product of the number of states and actions.\n",
        "\n",
        "### Attaining the Optimal Policy via Policy Iteration\n",
        "\n",
        "The initiation of the Policy Iteration algorithm involves the selection of an arbitrary policy. It subsequently engages in a cyclic process alternating between policy evaluation and policy improvement. In each iteration, the algorithm begins by evaluating the existing policy, determining the expected return for each state. Following this evaluation, it proceeds to enhance the policy by choosing actions that maximize the expected return for each state. This iterative sequence persists until the policy undergoes no further changes. At this juncture, the algorithm concludes that the policy has converged to its optimal state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4DcH5yJLXqH"
      },
      "source": [
        "<a name='1-2'></a>\n",
        "\n",
        "### Question 4:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XjSb1lX147hd"
      },
      "outputs": [],
      "source": [
        "class PolicyIteration:\n",
        "    def __init__(self, env, discount_factor, theta=1e-8):\n",
        "        self.env = env\n",
        "        self.discount_factor = discount_factor\n",
        "        self.theta = theta\n",
        "        self.reset()\n",
        "        self.state_values = (\n",
        "            np.ones((self.env.observation_space.n)) / self.env.action_space.n\n",
        "        )\n",
        "        self.q_values = (\n",
        "            np.ones((self.env.observation_space.n, self.env.action_space.n))\n",
        "            / self.env.action_space.n\n",
        "        )\n",
        "        self.state_values[self.env.observation_space.n - 1] = 0\n",
        "        self.q_values[self.env.observation_space.n - 1] = np.zeros(\n",
        "            (self.env.action_space.n)\n",
        "        )\n",
        "        self.policy = np.random.randint(\n",
        "            self.env.action_space.n, size=self.env.observation_space.n\n",
        "        )  # initial policy\n",
        "        self.policy_stable = False\n",
        "\n",
        "    def policy_evaluation(self):\n",
        "        self.delta = np.inf\n",
        "\n",
        "        while self.delta >= self.theta:\n",
        "            self.delta = 0\n",
        "\n",
        "            for state in range(self.env.observation_space.n):\n",
        "                v = self.state_values[state]\n",
        "\n",
        "                new_state_value = 0\n",
        "                for probability, next_state, reward, done in self.env.P[state][\n",
        "                    self.policy[state]\n",
        "                ]:\n",
        "                    new_state_value += probability * (\n",
        "                        reward + self.discount_factor *\n",
        "                        self.state_values[next_state]\n",
        "                    )\n",
        "\n",
        "                self.state_values[state] = new_state_value\n",
        "\n",
        "                self.delta = np.max(\n",
        "                    [self.delta, abs(v - self.state_values[state])])\n",
        "\n",
        "    def policy_improvement(self):\n",
        "        self.policy_stable = True\n",
        "\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            old_policy = self.policy[state]\n",
        "\n",
        "            for action in range(self.env.action_space.n):\n",
        "                action_value = 0\n",
        "\n",
        "                for probability, next_state, reward, done in self.env.P[state][action]:\n",
        "                    action_value += probability * (\n",
        "                        reward + self.discount_factor *\n",
        "                        self.state_values[next_state]\n",
        "                    )\n",
        "\n",
        "                self.q_values[state, action] = action_value\n",
        "\n",
        "            self.policy[state] = np.argmax(self.q_values[state, :])\n",
        "\n",
        "            if old_policy != self.policy[state]:\n",
        "                self.policy_stable = False\n",
        "\n",
        "    def policy_estimation(self):\n",
        "        self.policy_stable = False\n",
        "\n",
        "        while not self.policy_stable:\n",
        "            self.policy_evaluation()\n",
        "            self.policy_improvement()\n",
        "\n",
        "    def take_action(self, action):\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def get_optimal_policy(self, state):\n",
        "        return self.policy[state]\n",
        "\n",
        "    def get_state_values(self):\n",
        "        return self.state_values\n",
        "\n",
        "    def get_q_values(self):\n",
        "        return self.q_values\n",
        "\n",
        "    def reset(self):\n",
        "        initial_state = self.env.reset()\n",
        "        return initial_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4G-kVjmLYj4"
      },
      "source": [
        "<a name='1-3'></a>\n",
        "\n",
        "### Question 5:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A function we'll use to show the results of our algorithms as a visual map\n",
        "def visualize(optimalPolicy):\n",
        "    map = ''\n",
        "    for i in range(axes_states):\n",
        "        if i: \n",
        "            map += '\\n'\n",
        "        map += ((4 * axes_states) * '-' + '-\\n| ')\n",
        "        for j in range(axes_states):\n",
        "            act = action_symbols[optimalPolicy[axes_states * i + j]]\n",
        "            map += f'{act} | '\n",
        "    map += ('\\n' + (4 * axes_states) * '-' + '-')\n",
        "    print(map)\n",
        "\n",
        "\n",
        "\n",
        "# A function we'll use to print the results of our algorithms as a table\n",
        "def tabulate(optimalPolicy, decimal=4):\n",
        "    for index, x in enumerate(optimalPolicy):\n",
        "        if (index+1)%axes_states:\n",
        "            print(f'{x:.{decimal}f}', end='\\t')\n",
        "        else:\n",
        "            print(f'{x:.{decimal}f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB651-ZY4vjE"
      },
      "source": [
        "<a name='1-3-1'></a>\n",
        "\n",
        "#### Value Iteration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RGYLOfYAuKjY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "V(s):\n",
            "0.5905\t0.6561\t0.7290\t0.6561\n",
            "0.6561\t0.0000\t0.8100\t0.0000\n",
            "0.7290\t0.8100\t0.9000\t0.0000\n",
            "0.0000\t0.9000\t1.0000\t0.0000\n",
            "\n",
            "Q(s, a):\n",
            "0.5314\t0.5905\t0.5905\t0.5314\n",
            "0.5314\t0.0000\t0.6561\t0.5905\n",
            "0.5905\t0.7290\t0.5905\t0.6561\n",
            "0.6561\t0.0000\t0.5905\t0.5905\n",
            "0.5905\t0.6561\t0.0000\t0.5314\n",
            "0.0000\t0.0000\t0.0000\t0.0000\n",
            "0.0000\t0.8100\t0.0000\t0.6561\n",
            "0.0000\t0.0000\t0.0000\t0.0000\n",
            "0.6561\t0.0000\t0.7290\t0.5905\n",
            "0.6561\t0.8100\t0.8100\t0.0000\n",
            "0.7290\t0.9000\t0.0000\t0.7290\n",
            "0.0000\t0.0000\t0.0000\t0.0000\n",
            "0.0000\t0.0000\t0.0000\t0.0000\n",
            "0.0000\t0.8100\t0.9000\t0.7290\n",
            "0.8100\t0.9000\t1.0000\t0.8100\n",
            "0.0000\t0.0000\t0.0000\t0.0000\n",
            "\n",
            "Optimal Policy: (backend representation)\n",
            "1\t2\t1\t0\n",
            "1\t0\t1\t0\n",
            "2\t1\t1\t0\n",
            "0\t2\t2\t0\n",
            "\n",
            "Optimal Policy: (graphical representation)\n",
            "-----------------\n",
            "| ↓ | → | ↓ | ← | \n",
            "-----------------\n",
            "| ↓ | ← | ↓ | ← | \n",
            "-----------------\n",
            "| → | ↓ | ↓ | ← | \n",
            "-----------------\n",
            "| ← | → | → | ← | \n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='ansi', new_step_api=True)\n",
        "value_iteration = ValueIteration(env, discount_factor)\n",
        "value_iteration.value_estimation()\n",
        "\n",
        "V_s = value_iteration.get_state_values()\n",
        "print('V(s):')\n",
        "tabulate(V_s)\n",
        "print()\n",
        "\n",
        "Q_sa = value_iteration.get_q_values()\n",
        "print('Q(s, a):')\n",
        "for data in Q_sa: tabulate(data)\n",
        "print()\n",
        "\n",
        "PI_star = [value_iteration.get_optimal_policy(x) for x in range(total_states)]\n",
        "print('Optimal Policy: (backend representation)')\n",
        "tabulate(PI_star, 0)\n",
        "print()\n",
        "\n",
        "print('Optimal Policy: (graphical representation)')\n",
        "visualize(PI_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipZlzoXH40Mn"
      },
      "source": [
        "<a name='1-3-2'></a>\n",
        "\n",
        "#### Policy Iteration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7Vxf_xKc44QT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "V(s):\n",
            "0.5905\t0.6561\t0.7290\t0.6561\n",
            "0.6561\t0.0000\t0.8100\t0.0000\n",
            "0.7290\t0.8100\t0.9000\t0.0000\n",
            "0.0000\t0.9000\t1.0000\t0.0000\n",
            "\n",
            "Q(s, a):\n",
            "0.5314\t0.5905\t0.5905\t0.5314\n",
            "0.5314\t0.0000\t0.6561\t0.5905\n",
            "0.5905\t0.7290\t0.5905\t0.6561\n",
            "0.6561\t0.0000\t0.5905\t0.5905\n",
            "0.5905\t0.6561\t0.0000\t0.5314\n",
            "0.0000\t0.0000\t0.0000\t0.0000\n",
            "0.0000\t0.8100\t0.0000\t0.6561\n",
            "0.0000\t0.0000\t0.0000\t0.0000\n",
            "0.6561\t0.0000\t0.7290\t0.5905\n",
            "0.6561\t0.8100\t0.8100\t0.0000\n",
            "0.7290\t0.9000\t0.0000\t0.7290\n",
            "0.0000\t0.0000\t0.0000\t0.0000\n",
            "0.0000\t0.0000\t0.0000\t0.0000\n",
            "0.0000\t0.8100\t0.9000\t0.7290\n",
            "0.8100\t0.9000\t1.0000\t0.8100\n",
            "0.0000\t0.0000\t0.0000\t0.0000\n",
            "\n",
            "Optimal Policy: (backend representation)\n",
            "1\t2\t1\t0\n",
            "1\t0\t1\t0\n",
            "2\t1\t1\t0\n",
            "0\t2\t2\t0\n",
            "\n",
            "Optimal Policy: (graphical representation)\n",
            "-----------------\n",
            "| ↓ | → | ↓ | ← | \n",
            "-----------------\n",
            "| ↓ | ← | ↓ | ← | \n",
            "-----------------\n",
            "| → | ↓ | ↓ | ← | \n",
            "-----------------\n",
            "| ← | → | → | ← | \n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, new_step_api=True)\n",
        "policy_iteration = PolicyIteration(env, discount_factor)\n",
        "policy_iteration.policy_estimation()\n",
        "\n",
        "V_s = policy_iteration.get_state_values()\n",
        "print('V(s):')\n",
        "tabulate(V_s)\n",
        "print()\n",
        "\n",
        "Q_sa = policy_iteration.get_q_values()\n",
        "print('Q(s, a):')\n",
        "for data in Q_sa: tabulate(data)\n",
        "print()\n",
        "\n",
        "PI_star = [policy_iteration.get_optimal_policy(x) for x in range(total_states)]\n",
        "print('Optimal Policy: (backend representation)')\n",
        "tabulate(PI_star, 0)\n",
        "print()\n",
        "\n",
        "print('Optimal Policy: (graphical representation)')\n",
        "visualize(PI_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3fnAFqJLpVI"
      },
      "source": [
        "<a name='1-4'></a>\n",
        "\n",
        "### Question 6:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal here is to find the average time consumption of the two utilized algorithms: *Value Iteration* and *Policy Iteration* methods. To do so, we run each of them for 50 episodes and capture each one's total runtime. By dividing the total runtime by number of running episodes (50), we'll get the mean runtime period of each algorithm. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JuoMPH_PAcv"
      },
      "source": [
        "<a name='1-4-1'></a>\n",
        "\n",
        "#### Value Iteration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vCBnAicAPAcv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average execution time for Value Iteration: \u001b[1;34m12.75904\u001b[0m milliseconds\n"
          ]
        }
      ],
      "source": [
        "class color:            # Customize the output's text format in print()\n",
        "    BOLD = '\\033[1;34m' # Set bold and blue text via ANSI escape sequences\n",
        "    END = '\\033[0m'     # Reset text format via ANSI escape sequences\n",
        "    \n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='ansi', new_step_api=True)\n",
        "\n",
        "TOTAL = 0\n",
        "for _ in range(50):\n",
        "    value_iteration = ValueIteration(env, discount_factor)\n",
        "    start = time.time_ns()\n",
        "    value_iteration.value_estimation()\n",
        "    end = time.time_ns()\n",
        "    TOTAL += (end - start)\n",
        "env.close()\n",
        "\n",
        "avg = TOTAL / 50 / 10**6\n",
        "print(f'Average execution time for Value Iteration: {color.BOLD}{avg}{color.END} milliseconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1BMk-BfPAcv"
      },
      "source": [
        "<a name='1-4-2'></a>\n",
        "\n",
        "#### Policy Iteration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "LnUZpvLP446n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average execution time for Policy Iteration: \u001b[1;34m63.057256\u001b[0m milliseconds\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='ansi', new_step_api=True)\n",
        "\n",
        "TOTAL = 0\n",
        "for _ in range(50):\n",
        "    policy_iteration = PolicyIteration(env, discount_factor)\n",
        "    start = time.time_ns()\n",
        "    policy_iteration.policy_estimation()\n",
        "    end = time.time_ns()\n",
        "    TOTAL += (end - start)\n",
        "env.close()\n",
        "\n",
        "avg = TOTAL / 50 / 10**6\n",
        "print(f'Average execution time for Policy Iteration: {color.BOLD}{avg}{color.END} milliseconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison of Value Iteration and Policy Iteration\n",
        "As seen in the previous question, both policy iteration and value iteration algorithms converge to the same optimal policy, which maximizes rewards in the environment.\n",
        "\n",
        "Value iteration algorithm only requires policy evaluation in each iteration, as it updates the policy directly from the values. On the other hand, policy iteration algorithm needs to perform both policy evaluation and policy improvement in each iteration. This difference in the number of steps required per iteration leads to a difference in the overall convergence speed of the two algorithms. Value iteration algorithm generally converges faster than policy iteration algorithm. This conclusion is also supported in practice from the results obtained in this question. However, it is important to note that the convergence speed of both algorithms can be affected by a number of factors, such as the size of the state space, the number of actions, and the discount factor. In some rare cases, policy iteration algorithm may converge faster than value iteration algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLtPLm-ELpG9"
      },
      "source": [
        "<a name='2'></a>\n",
        "\n",
        "## Part 2: Q-Learning Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "152\n",
            "you can see the environment in each step by render command\n",
            "1 2 3 0\n",
            "500\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "# Defining the hyperparameters of the problem\n",
        "REPS = 20\n",
        "EPISODES = 2000\n",
        "EPSILON = 0.1\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.9\n",
        "STUDENT_NUM = 269  # 810699269\n",
        "ACTION_SPACE = {0: \"DOWN\", 1: \"UP\", 2: \"RIGHT\", 3: \"LEFT\", 4: \"PICKUP\", 5: \"DROPOFF\"}\n",
        "ACTION_SYMBOLS = [\"↓\", \"↑\", \"→\", \"←\", \"P\", \"D\"]\n",
        "\n",
        "# Introducing the environment of the problem\n",
        "env = gym.make(\"Taxi-v3\", new_step_api=True)\n",
        "# env.seed(seed=STUDENT_NUM)    [This line is replaced with the following line to avoid WARNINGs]\n",
        "Initial_State = env.reset(seed=STUDENT_NUM)\n",
        "print(Initial_State)\n",
        "\n",
        "# Getting familiar with the environment\n",
        "print(\"you can see the environment in each step by render command\")\n",
        "# env.render()\n",
        "taxi_row, taxi_col, pass_idx, dest_idx = env.decode(Initial_State)\n",
        "print(taxi_row, taxi_col, pass_idx, dest_idx)\n",
        "\n",
        "# Total no. of states\n",
        "print(env.observation_space.n)\n",
        "\n",
        "# Total no. of actions\n",
        "print(env.action_space.n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7yRce6rSlhY"
      },
      "source": [
        "<a name='2-0'></a>\n",
        "\n",
        "### Question 7:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlpNdHs8SwAk"
      },
      "source": [
        "Q-learning is a reinforcement learning algorithm that finds the optimal policy for an agent. It is model-free, value-based, and off-policy, meaning that it does not require a model of the environment. So, it estimates the value of each state-action pair, and learns from a policy different from the one being executed. The algorithm updates the Q-values, which represent the expected future rewards for taking a particular action in a special state, to maximize the total rewards obtained from the environment.\n",
        "\n",
        "The Q-learning algorithm follows these steps:\n",
        "1. **Initialize the Q-table:** The Q-table is a matrix that stores the Q-values for each state-action pair. Initially, all Q-values are set to zero or random values.\n",
        "2. **Observation:** The agent observes the current state of the environment.\n",
        "3. **Action selection:** The agent selects an action to perform based on the current state. This can be done using an exploration-exploitation trade-off strategy, where the agent explores new actions with a certain probability and exploits its current knowledge with the remaining probability.\n",
        "4. **Perform the action:** The agent performs the selected action in the environment.\n",
        "5. **Receive reward and new state:** The agent receives a reward from the environment based on the action taken and transitions to a new state.\n",
        "6. **Update the Q-value:** The Q-value for the previous state-action pair is updated using the following formula:\n",
        "\n",
        "$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)] $$\n",
        "\n",
        "where:\n",
        "- $Q(s, a)$ is the Q-value for state $s$ and action $a$,\n",
        "- $\\alpha$ is the learning rate, \n",
        "- $r$ is the reward received, \n",
        "- $\\gamma$ is the discount factor, \n",
        "- $s'$ is the new state, and \n",
        "- $\\max(Q(s', a'))$ is the maximum Q-value for the new state.\n",
        "\n",
        "7. **Iteration:** Repeat steps 2-6 until the agent reaches a terminal state or a predefined number of iterations.\n",
        "\n",
        "The algorithm learns the optimal policy by iteratively updating the Q-values, which represent the expected future rewards for taking a particular action in a special state. The optimal policy is obtained by selecting the action with the highest Q-value for each state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZJAP4nMLpiZ"
      },
      "source": [
        "<a name='2-1'></a>\n",
        "\n",
        "### Question 8:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, env, epsilon, learning_rate, discount_factor, seed, epsilon_decay, learning_rate_decay):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon\n",
        "        self.learning_rate = learning_rate\n",
        "        # self.olr = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "        self.seed = seed\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.learning_rate_decay = learning_rate_decay\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        prob = np.random.rand()\n",
        "        if prob < self.epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            action = self.get_optimal_policy(state)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update_q_table(self, state, action, nextState, reward):\n",
        "        Q_old = self.q_table[state][action]\n",
        "        Q_new = Q_old + self.learning_rate * \\\n",
        "            (reward + self.discount_factor *\n",
        "             np.max(self.q_table[int(nextState), :]) - Q_old)\n",
        "\n",
        "        self.q_table[state][action] = Q_new\n",
        "\n",
        "    def decay_epsilon(self, episode):\n",
        "        self.epsilon = np.exp(-self.epsilon_decay*(episode+1))\n",
        "\n",
        "    def decrease_learning_rate(self, episode):\n",
        "\n",
        "        self.learning_rate = 0.99 * \\\n",
        "            np.exp(-self.learning_rate_decay * (episode + 1))\n",
        "\n",
        "    def take_action(self, action):\n",
        "\n",
        "        next_state, reward, done, _, _ = self.env.step(action)\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def get_optimal_policy(self, state):\n",
        "\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def get_q_values(self):\n",
        "\n",
        "        return self.q_table\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        # self.learning_rate = self.olr\n",
        "\n",
        "        return self.env.reset(seed=self.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<style>\n",
        "    .equation{\n",
        "        color:black; \n",
        "        display:flex; \n",
        "        align-items:center; \n",
        "        justify-content:center; \n",
        "        background-color:SeaShell; \n",
        "        border-radius:10px; \n",
        "        height:65px; \n",
        "        font-size:20px;\n",
        "    }\n",
        "</style>\n",
        "\n",
        "In the `decay_epsilon` function of this class, the value of epsilon is decreased. In the first episode, the value of epsilon is equal to 1. In subsequent episodes, the value of epsilon is equal to the following relationship:\n",
        "\n",
        "In this class, the `decay_epsilon` function is used when the value of epsilon is needed to be decreased over time. The value of epsilon is initially set to 1 in the first episode, and then updated using the following formula in subsequent episodes:\n",
        "<div class=\"equation\">  \n",
        "<p>\n",
        "\n",
        "$ Epsilon = e^{- (\\text{epsilon rate decay} \\times \\text{episode number})} $\n",
        "\n",
        "</p>\n",
        "</div>\n",
        "\n",
        "Where the value of _epsilon decay rate_ would be set to 0.003 as the `epsilon_decay` argument of the class.\n",
        "\n",
        "Also, In the `decrease_learning_rate` function of this class, the learning rate is decreased over time. The learning rate is initially set to 0.99 in the first episode, and then updated using the following formula in subsequent episodes:\n",
        "\n",
        "<div class=\"equation\">  \n",
        "<p>\n",
        "\n",
        "$ Learning Rate = 0.99 \\times e^{- (\\text{learning rate decay} \\times \\text{episode number})} $\n",
        "\n",
        "</p>\n",
        "</div>\n",
        "\n",
        "Where the value of _learning decay rate_ would be set to 0.0023 as the `learning_rate_decay` argument of the class.\n",
        "\n",
        "In the next question, a function is developed to find the reward obtained from executing the optimal policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5HFAMk-Lpvs"
      },
      "source": [
        "<a name='2-2'></a>\n",
        "\n",
        "### Question 9:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function repeats the Q-learning algorithm 20 times and runs it for 2000 episodes, depending on the learning rate, and returns the rewards received and the reward of the optimal policy in the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def RunQLearningAgent(is_decay_epsilon: bool, is_decay_learning_rate: bool):\n",
        "    cumulative_reward = np.zeros((REPS, EPISODES))\n",
        "    optimal_cumulative_reward = -np.inf\n",
        "    optimal_agent = None\n",
        "\n",
        "    if is_decay_learning_rate:\n",
        "        learning_rate = 0.99\n",
        "    else:\n",
        "        learning_rate = LEARNING_RATE\n",
        "\n",
        "    for rep in range(REPS):\n",
        "        agent = QLearningAgent(env, 1, learning_rate,\n",
        "                               DISCOUNT, STUDENT_NUM, 0.003, 0.0023)\n",
        "        # QLearningAgent(env, epsilon, learning_rate, discount_factor, seed, epsilon_decay, learning_rate_decay)\n",
        "\n",
        "        for episode in range(EPISODES):\n",
        "            Initial_state = agent.reset()\n",
        "            rewards = []\n",
        "\n",
        "            while True:\n",
        "                # action = np.random.choice(list(ACTION_SPACE.keys()))\n",
        "                # next_state, rew, done, _ = agent.step(bestAction)\n",
        "                action = agent.choose_action(Initial_state)\n",
        "                next_state, rew, done = agent.take_action(action)\n",
        "                agent.update_q_table(Initial_state, action, next_state, rew)\n",
        "                Initial_state = next_state\n",
        "                rewards.append(rew)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            cumulative_reward[rep][episode] = np.sum(rewards)\n",
        "            if is_decay_epsilon:\n",
        "                agent.decay_epsilon(episode+1)\n",
        "            if is_decay_learning_rate:\n",
        "                agent.decrease_learning_rate(episode+1)\n",
        "\n",
        "        Initial_state = agent.reset()\n",
        "        rewards = []\n",
        "        while True:\n",
        "            action = agent.get_optimal_policy(Initial_state)\n",
        "            next_state, rew, done = agent.take_action(action)\n",
        "            Initial_state = next_state\n",
        "            rewards.append(rew)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        ans = np.sum(rewards)\n",
        "\n",
        "        optimal_cumulative_reward = max(optimal_cumulative_reward, ans)\n",
        "        optimal_agent = agent\n",
        "\n",
        "    return cumulative_reward, optimal_cumulative_reward, optimal_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\Python\\env\\venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Attaining the optimal policies\n",
        "total_reward_fixed_learning, optimal_policy_fixed, optimal_agent_fixed = RunQLearningAgent(is_decay_epsilon=True, is_decay_learning_rate=False)\n",
        "total_reward_decreasing_learning, optimal_policy_dynamic, optimal_agent_dynamic = RunQLearningAgent(is_decay_epsilon=True, is_decay_learning_rate=True)\n",
        "\n",
        "# Visualizing the results\n",
        "plt.figure(figsize=(14,8))\n",
        "plt.title('Q-Learning Algorithm')\n",
        "plt.plot(np.convolve(np.mean(total_reward_fixed_learning, axis=0), np.ones(40)/40, mode='valid'), label=r'$\\alpha=0.1$')\n",
        "plt.plot(np.convolve(np.mean(total_reward_decreasing_learning, axis=0), np.ones(40)/40, mode='valid'), label=r'$\\alpha=decayed$')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison of the Results\n",
        "Using a plot, the comparison of the results from the algorithm execution with two learning rate modes, i.e. fixed and decreasing, gets easier. \n",
        "It can be seen that both cases of the algorithm eventually converge to the optimal policy, but the case where the learning rate is decreasing has a faster convergence speed and converges to the optimal policy sooner. It's crucial to note that the difference, in this problem, is **not** dramatically huge. That might be due to the simplicity and smallness of the environment we're dealing with. Additionally, the case where the learning rate is decreasing has less regret than the case where the learning rate is constant, and the amount of reward received in each episode is higher. The superiority of the decreasing learning rate case is due to the fact that the agent is trained with a high learning rate at the beginning and gradually decreases the learning rate. This allows the agent to learn a lot from new data at the beginning and rely more on its own experience as its experience increases. As a result, the convergence speed increases and the regret decreases. However, in the case where the learning rate is equal to a small and constant value, the agent's training is slow, resulting in a lower convergence speed and higher regret.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVcMKEGDQWdU"
      },
      "source": [
        "<a name='2-3'></a>\n",
        "\n",
        "### Question 10:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this question, the graphical render of optimal policy is shown using *gym* library's built-in graphical functions for this problem. All we need to do is to pass the **optimal policy** we obtained from previous question to the `env.step()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Agent with Fixed Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FlHPV0kqQWdU"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Taxi-v3', render_mode='human', new_step_api=True)\n",
        "pos = env.reset(seed=STUDENT_NUM)\n",
        "while (True):\n",
        "    (pos, _, terminated, _, _) = env.step(\n",
        "        optimal_agent_fixed.get_optimal_policy(pos))\n",
        "    time.sleep(0.5)\n",
        "    if (terminated):\n",
        "        break\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Agent with Decreasing Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('Taxi-v3', render_mode='human', new_step_api=True)\n",
        "pos = env.reset(seed=STUDENT_NUM)\n",
        "while (True):\n",
        "    (pos, _, terminated, _, _) = env.step(\n",
        "        optimal_agent_dynamic.get_optimal_policy(pos))\n",
        "    time.sleep(0.5)\n",
        "    if (terminated):\n",
        "        break\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've successfully witnessed the graphical result, it's time to examine each mode's average runtime, so we can have a better comparison between them on time-consumption territory as well. In order to avoid repeated code, here's a mutual function to measure the average time consumption of each algorithm for 50 episodes of execution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class color:            # Customize the output's text format in print()\n",
        "    BOLD = '\\033[1;34m' # Set bold and blue text via ANSI escape sequences\n",
        "    END = '\\033[0m'     # Reset text format via ANSI escape sequences\n",
        "    UNDER = '\\033[4m'   # Set underlined text via ANSI escape sequences\n",
        "\n",
        "\n",
        "def timer(agent, mode:str):\n",
        "    start = time.time_ns()\n",
        "    for _ in range(50):\n",
        "        state = agent.reset()\n",
        "        rewards = []\n",
        "        env.render()\n",
        "\n",
        "        while True:\n",
        "            action = agent.get_optimal_policy(state)\n",
        "            next_state, reward, done = agent.take_action(action)\n",
        "            state = next_state\n",
        "            rewards.append(reward)\n",
        "            env.render()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "    end = time.time_ns()\n",
        "    avg = (end - start) / 50 / 10**6\n",
        "\n",
        "    print(f'Average execution time for {color.UNDER}agent with {mode}{color.END}: {color.BOLD}{avg}{color.END} milliseconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Agent with Fixed Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average execution time for \u001b[4magent with fixed learning rate\u001b[0m: \u001b[1;34m1.540214\u001b[0m milliseconds\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Taxi-v3', render_mode='ansi')\n",
        "env.reset()\n",
        "timer(optimal_agent_fixed, 'fixed learning rate')\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Agent with Decreasing Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average execution time for \u001b[4magent with decreasing learning rate\u001b[0m: \u001b[1;34m1.539988\u001b[0m milliseconds\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Taxi-v3', render_mode='ansi')\n",
        "env.reset()\n",
        "timer(optimal_agent_dynamic, 'decreasing learning rate')\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a final section of [Part 2: Q-Learning Algorithm](#part-2-q-learning-algorithm), we intend to manually visualize the map and the path our agent travels in it with a rough simple string map in terminal. To do so, we need to get the result for **optimal_policy** our agent has found in previous questions. By saving this *optimal_policy* in a variable named *path*, we'll see that it consists of a chain of 13 states our agent travels. Each state is represented by an integer number that will be shown as the output of the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def raw_map():\n",
        "    # Raw map of my problem\n",
        "    map = \\\n",
        "        [['-','-','-','-','-','-','-','-','-','-','-'],\n",
        "        ['|','H',':',':','|',':',':',':',':','G','|'],\n",
        "        ['|',':',':',':','|',':',':',':',':',':','|'],\n",
        "        ['|',':',':',':',':',':',':',':',':',':','|'],\n",
        "        ['|',':','|',':',':',':','|',':',':',':','|'],\n",
        "        ['|','Y','|',':',':',':','|','B',':',':','|'],\n",
        "        ['-','-','-','-','-','-','-','-','-','-','-'],\n",
        "        ]\n",
        "    print('Raw Map without agent\\'s presence:')\n",
        "    # print(np.array(map), '\\n')\n",
        "\n",
        "    string = ''\n",
        "    for i in map:\n",
        "        for j in i:\n",
        "            if j == ':':\n",
        "                j = ' '\n",
        "            string += j + '  '\n",
        "        string += '\\n'\n",
        "    print(string)\n",
        "    return map\n",
        "\n",
        "\n",
        "def visualizer(agent, map):\n",
        "    state = agent.reset()\n",
        "    path = []\n",
        "\n",
        "    while True:\n",
        "        path.append(state)\n",
        "        action = agent.get_optimal_policy(state)\n",
        "        next_state, reward, done = agent.take_action(action)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    print(f'The optimal_policy is a chain of length 13, consisting of these states: \\n{path}')\n",
        "\n",
        "    for coord in path:\n",
        "        coord = str(coord)\n",
        "        if len(coord) == 2:\n",
        "            coord = '0' + coord\n",
        "        x = int(coord[1])\n",
        "        y = int(coord[0]) + 1\n",
        "        map[y][x] = '0'\n",
        "    print('Map with the agent\\'s optimal path:')\n",
        "    # print(np.array(map), '\\n')\n",
        "\n",
        "    string = ''\n",
        "    for i in map:\n",
        "        for j in i:\n",
        "            if j == ':':\n",
        "                j = ' '\n",
        "            elif j == '0':\n",
        "                j = '\\033[1;31m' + '0' + '\\033[0m'\n",
        "            string += j + '  '\n",
        "        string += '\\n'\n",
        "    print(string)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw Map without agent's presence:\n",
            "-  -  -  -  -  -  -  -  -  -  -  \n",
            "|  H        |              G  |  \n",
            "|           |                 |  \n",
            "|                             |  \n",
            "|     |           |           |  \n",
            "|  Y  |           |  B        |  \n",
            "-  -  -  -  -  -  -  -  -  -  -  \n",
            "\n",
            "--- \u001b[1;34mAgent with Fixed Learning Rate\u001b[0m ---\n",
            "The optimal_policy is a chain of length 13, consisting of these states: \n",
            "[152, 252, 272, 372, 472, 476, 376, 276, 256, 236, 136, 116, 16]\n",
            "Map with the agent's optimal path:\n",
            "-  -  -  -  -  -  -  -  -  -  -  \n",
            "|  \u001b[1;31m0\u001b[0m        |              G  |  \n",
            "|  \u001b[1;31m0\u001b[0m     \u001b[1;31m0\u001b[0m  |  \u001b[1;31m0\u001b[0m              |  \n",
            "|        \u001b[1;31m0\u001b[0m     \u001b[1;31m0\u001b[0m     \u001b[1;31m0\u001b[0m        |  \n",
            "|     |           |  \u001b[1;31m0\u001b[0m        |  \n",
            "|  Y  |           |  \u001b[1;31m0\u001b[0m        |  \n",
            "-  -  -  -  -  -  -  -  -  -  -  \n",
            "\n",
            "--- \u001b[1;34mAgent with Decreasing Learning Rate\u001b[0m ---\n",
            "The optimal_policy is a chain of length 13, consisting of these states: \n",
            "[152, 252, 272, 372, 472, 476, 376, 276, 256, 236, 136, 36, 16]\n",
            "Map with the agent's optimal path:\n",
            "-  -  -  -  -  -  -  -  -  -  -  \n",
            "|  \u001b[1;31m0\u001b[0m     \u001b[1;31m0\u001b[0m  |              G  |  \n",
            "|  \u001b[1;31m0\u001b[0m     \u001b[1;31m0\u001b[0m  |  \u001b[1;31m0\u001b[0m              |  \n",
            "|        \u001b[1;31m0\u001b[0m     \u001b[1;31m0\u001b[0m     \u001b[1;31m0\u001b[0m        |  \n",
            "|     |           |  \u001b[1;31m0\u001b[0m        |  \n",
            "|  Y  |           |  \u001b[1;31m0\u001b[0m        |  \n",
            "-  -  -  -  -  -  -  -  -  -  -  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "RAW = raw_map()\n",
        "print(f'--- {color.BOLD}Agent with Fixed Learning Rate{color.END} ---')\n",
        "visualizer(optimal_agent_fixed, RAW)\n",
        "\n",
        "print(f'--- {color.BOLD}Agent with Decreasing Learning Rate{color.END} ---')\n",
        "visualizer(optimal_agent_dynamic, RAW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be seen that in this problem, the number of steps the agent must travel doesn't differ based on the two discussed modes, i.e. *fixed learning rate* or *decreasing learning rate*. However, the path itself does differ, and the difference is shown in the maps above. The path which is printed with red zeroes, is obtained from decoding the states. The first and second digits of each state represent the **y** and **x** coordinates of the agent at that state, respectively. The third digit also shows if the passenger is on-board or not, which we didn't need in the map representations above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color:azure; border-radius:15px; height:150px; display: flex; align-items: center; \">\n",
        "\n",
        "<h4 style=\"color:grey; margin-left:8%; vertical-align:middle; \">by: <br> Mohammad Montazeri\n",
        "<br>\n",
        "810699269 </h4>\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
